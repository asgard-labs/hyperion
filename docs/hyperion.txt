-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


@package hyperion
@version 0.1.0.0

module Hyperion.CallClosure

-- | The purpose of this module is to generalize <a>call</a> from
--   <a>Process</a> so that it takes a 'Closure (SerializableDict a)'
--   instead of a 'Static (SerializableDict a)'. Note that this is a strict
--   generalization because any 'Static a' can be turned into 'Closure a'
--   via <a>staticClosure</a>, while a 'Closure a' cannot be turned into a
--   'Static a' in general.
--   
--   Note: The extra flexibility afforded by call' is needed in conjunction
--   with the 'Hyperion.Static (KnownNat j)' instance. In that case, we
--   cannot construct a 'Control.Distributed.Static.Static (Dict (KnownNat
--   j))', but we can construct a 'Closure (Dict (KnownNat j))'. NB: The
--   name <a>Static</a> is used in two places: <a>Static</a> and
--   <a>Static</a>. The former is a datatype and the latter is a typeclass.
--   
--   Most of the code here has been copied from <a>Process</a> and
--   <a>Closure</a>, with small modifications.
--   
--   <tt>CP</tt> version of <a>send</a> that uses a 'Closure
--   (SerializableDict a)' instead of 'Static (SerializableDict a)'
cpSend' :: forall a. Closure (SerializableDict a) -> ProcessId -> Closure (a -> Process ())

-- | <a>call</a> that uses a 'Closure (SerializableDict a)' instead of a
--   'Static (SerializableDict a)'.
call' :: (Binary a, Typeable a) => Closure (SerializableDict a) -> NodeId -> Closure (Process a) -> Process a


-- | This module contains copies of several utilities from
--   <a>Control.Concurrent.Async</a> from the <tt>async</tt> package, with
--   <a>IO</a> replaced by <a>Process</a>.
module Hyperion.Concurrent

-- | Runs the two <a>Process</a>es concurrently and returns the first
--   available result, <a>kill</a>ing the unfinished process. If any of the
--   processes throw an exception, the processes are <a>kill</a>ed and the
--   exception is propagated out of <a>race</a>.
race :: Process a -> Process b -> Process (Either a b)

-- | Runs the two <a>Process</a>es concurrently and returns both results.
--   If any of the processes throw an exception, the processes are
--   <a>kill</a>ed and the exception is propagated out of
--   <a>concurrently</a>.
concurrently :: Process a -> Process b -> Process (a, b)

-- | Runs two <a>Process</a>es concurrently in two new threads. Each
--   process will compute the result and <a>putMVar</a> it into an
--   <a>MVar</a>. The user-supplied continuation is applied to this
--   <a>MVar</a> concurrently with the two threads. When the continuation
--   returns a result, the new threads are <a>kill</a>ed and the result is
--   returned from <a>concurrently'</a>. If a thread fails, the <a>MVar</a>
--   is filled with <a>Left</a> <a>SomeException</a>.
--   
--   Note that the continutation can inspect the results of both threads by
--   emptying the <a>MVar</a> when appropriate.
--   
--   TODO: This code was originally copied from the
--   <a>Control.Concurrent.Async</a> module, with <a>forkIO</a> replaced by
--   <a>spawnLocal</a>. As of <tt>async-2.1</tt>, the code for this
--   function has changed. Have a look and figure out why, and whether the
--   changes should be ported here?
concurrently' :: Process a -> Process b -> (MVar (Either SomeException (Either a b)) -> Process r) -> Process r
newtype Concurrently m a
Concurrently :: m a -> Concurrently m a
[runConcurrently] :: Concurrently m a -> m a

-- | Run several computations in a traversable structure concurrently and
--   collect the results.
doConcurrently :: (Applicative (Concurrently m), Traversable t) => t (m a) -> m (t a)

-- | Run several computations in a traversable structure concurrently and
--   forget the results.
doConcurrently_ :: (Applicative (Concurrently m), Foldable t, Functor t) => t (m a) -> m ()

-- | Concurrently map a function over a traversable structure.
mapConcurrently :: (Applicative (Concurrently m), Traversable t) => (a -> m b) -> t a -> m (t b)

-- | Concurrently map a function over a traversable structure and forget
--   the results.
mapConcurrently_ :: (Applicative (Concurrently m), Foldable t, Functor t) => (a -> m b) -> t a -> m ()

-- | Flipped version of <a>mapConcurrently</a>.
forConcurrently :: (Applicative (Concurrently m), Traversable t) => t a -> (a -> m b) -> m (t b)

-- | Flipped version of <a>mapConcurrently_</a>.
forConcurrently_ :: (Applicative (Concurrently m), Foldable t, Functor t) => t a -> (a -> m b) -> m ()

-- | Concurrently run <tt>n</tt> copies of a computation and collect the
--   results in a list.
replicateConcurrently :: Applicative (Concurrently m) => Int -> m a -> m [a]

-- | Concurrently run <tt>n</tt> copies of a computation and forget the
--   results.
replicateConcurrently_ :: Applicative (Concurrently m) => Int -> m a -> m ()
instance GHC.Base.Functor m => GHC.Base.Functor (Hyperion.Concurrent.Concurrently m)
instance GHC.Base.Applicative (Hyperion.Concurrent.Concurrently Control.Distributed.Process.Internal.Types.Process)
instance GHC.Base.Alternative (Hyperion.Concurrent.Concurrently Control.Distributed.Process.Internal.Types.Process)
instance (GHC.Base.Functor m, GHC.Base.Applicative (Hyperion.Concurrent.Concurrently m)) => GHC.Base.Applicative (Hyperion.Concurrent.Concurrently (Control.Monad.Trans.Reader.ReaderT r m))
instance (GHC.Base.Functor m, GHC.Base.Alternative (Hyperion.Concurrent.Concurrently m)) => GHC.Base.Alternative (Hyperion.Concurrent.Concurrently (Control.Monad.Trans.Reader.ReaderT r m))
instance (GHC.Base.Applicative (Hyperion.Concurrent.Concurrently m), GHC.Base.Semigroup a) => GHC.Base.Semigroup (Hyperion.Concurrent.Concurrently m a)
instance (GHC.Base.Applicative (Hyperion.Concurrent.Concurrently m), GHC.Base.Semigroup a, GHC.Base.Monoid a) => GHC.Base.Monoid (Hyperion.Concurrent.Concurrently m a)

module Hyperion.Log
showText :: Show a => a -> Text
prettyShowText :: Show a => a -> Text
rawText :: MonadIO m => Text -> m ()

-- | Outputs the first argument to log. Prepends current time in the format
--   <tt>[%a %D %X]</tt> where <tt>%a</tt> is day of the week, <tt>%D</tt>
--   is date in <tt>mm/dd/yy</tt> format, <tt>%X</tt> is current time of
--   day in some default locale.
text :: MonadIO m => Text -> m ()

-- | Outputs a string to log using <a>text</a> where the string is a pretty
--   version of the first two arguments
info :: (Show a, MonadIO m) => Text -> a -> m ()

-- | Same as <a>info</a> but prepended by "WARN: ".
warn :: (Show a, MonadIO m) => Text -> a -> m ()

-- | Shorthand for <tt><a>info</a> "ERROR"</tt>
err :: (Show a, MonadIO m) => a -> m ()

-- | Same as <a>throwM</a> but first logs the error using <a>err</a>
throw :: (MonadThrow m, MonadIO m, Exception e) => e -> m a

-- | Same as <a>error</a> but first logs the error using <a>text</a> by
--   prepending "ERROR: " to the first argument.
throwError :: MonadIO m => String -> m a
flush :: IO ()
currentLogFile :: IORef (Maybe FilePath)
getLogFile :: MonadIO m => m (Maybe FilePath)

-- | Redirects log output to file by rewrting <a>stdout</a> and
--   <a>stderr</a> handles.
redirectToFile :: FilePath -> IO ()

module Hyperion.HoldServer
type HoldApi = "retry" :> Capture "service" Text :> Get '[JSON] (Maybe Text) :<|> "retry-all" :> Get '[JSON] [Text] :<|> "list" :> Get '[JSON] [Text]
newtype HoldMap
HoldMap :: TVar (Map Text (MVar ())) -> HoldMap
newHoldMap :: IO HoldMap
server :: HoldMap -> Server HoldApi

-- | Start a hold associated to the given service. Returns an IO action
--   that blocks until the hold is released
blockUntilRetried :: MonadIO m => HoldMap -> Text -> m ()

-- | Start the hold server on an available port and pass the port number to
--   the given action. The server is killed after the action finishes.
withHoldServer :: HoldMap -> (Int -> IO a) -> IO a

module Hyperion.Slurm.Environment

-- | Returns number of tasks per node by reading system environment
--   variables. If <tt>SLURM_NTASKS_PER_NODE</tt> is defined, returns it.
--   Otherwise, tries to compute from <tt>SLURM_NTASKS</tt> and
--   <tt>SLURM_JOB_NUM_NODES</tt>. If this doens't work either, fails with
--   <a>error</a>.
getNTasksPerNode :: IO Int

-- | Returns the contents of <tt>SLURM_JOB_NODELIST</tt> as a list of nodes
--   names
getJobNodes :: IO [String]

-- | Returns the value of <tt>SLURMD_NODENAME</tt>
lookupHeadNode :: IO (Maybe String)

module Hyperion.Slurm.JobId

-- | Type for job id. A job can be idetified by id or by name, hence the
--   two construtors.
data JobId
JobId :: Text -> JobId
JobName :: Text -> JobId
instance GHC.Classes.Ord Hyperion.Slurm.JobId.JobId
instance GHC.Classes.Eq Hyperion.Slurm.JobId.JobId
instance GHC.Show.Show Hyperion.Slurm.JobId.JobId

module Hyperion.TokenPool

-- | A <a>TokenPool</a> keeps track of the number of resources of some
--   kind, represented by "tokens". 'TokenPool (Just var)' indicates a
--   limited number of tokens, and <tt>var</tt> contains the number of
--   available tokens. When <tt>var</tt> contains 0, processes wishing to
--   use a token must block until one becomes available (see
--   <a>withToken</a>). 'TokenPool Nothing' represents an unlimited number
--   of tokens.
newtype TokenPool
TokenPool :: Maybe (TVar Int) -> TokenPool

-- | Create a new <a>TokenPool</a> containing the given number of tokens.
--   <a>Nothing</a> indicates an unlimited pool.
newTokenPool :: Maybe Int -> IO TokenPool

-- | Remove a token from the pool, run the given process, and then replace
--   the token. If no token is initially available, block until one becomes
--   available.
withToken :: (MonadIO m, MonadMask m) => TokenPool -> m a -> m a

module Hyperion.Util

-- | An opaque type representing a unique object. Only guaranteed to be
--   unique in one instance of a running program. For example, if we
--   allowed Unique's to be serialized and sent across the wire, or stored
--   and retrieved from a database, they would no longer be guaranteed to
--   be unique.
newtype Unique
MkUnique :: Integer -> Unique
uniqueSource :: IORef Integer

-- | Get a new Unique.
newUnique :: IO Unique

-- | <a>IO</a> action that returns a random string of given length
randomString :: Int -> IO String

-- | <tt>retryRepeated n doTry m</tt> tries to run <tt>doTry m</tt> n-1
--   times, after which it runs <tt>m</tt> 1 time. After each failure waits
--   15-90 seconds randomly. Returns on first success. Failure is
--   represented by a <a>Left</a> value.
retryRepeated :: (Show e, MonadIO m) => Int -> (m a -> m (Either e a)) -> m a -> m a
data WaitRetry e
WaitRetry :: e -> Int -> WaitRetry e
[err] :: WaitRetry e -> e
[waitTime] :: WaitRetry e -> Int

-- | <tt>retryExponential doTry m</tt> tries to run <tt>doTry m</tt>. After
--   the n-th successive failure, it waits time 2^n*t0, where t0 is a
--   randomly chosen time between 10 and 20 seconds. Unlike
--   <a>retryRepeated</a>, <a>retryExponential</a> never eventually throws
--   an exception, so it should only be used when the only way to recover
--   from the exception without the whole program crashing is to retry
--   until things work. Typically this means it should only be used in the
--   master process.
retryExponential :: MonadIO m => (m a -> m (Either e a)) -> (WaitRetry e -> m ()) -> m a -> m a

-- | Send an email to <tt>toAddr</tt> with the given <tt>subject</tt> and
--   <tt>body</tt>.
emailMessage :: MonadIO m => Text -> Text -> Text -> m ()

-- | Send an email to <tt>toAddr</tt> showing the object <tt>a</tt>. The
--   subject line is "msg: ...", where <tt>msg</tt> is the first argument
--   and "..." is the first 40 characters of 'show a'.
email :: (Show a, MonadIO m) => Text -> Text -> a -> m ()

-- | Send an email with msg <a>Error</a>
emailError :: (Show a, MonadIO m) => Text -> a -> m ()

-- | Takes a path and a list of <a>String</a> arguments, shell-escapes the
--   arguments, and combines everything into a single string.
shellEsc :: FilePath -> [String] -> String
minute :: NominalDiffTime
hour :: NominalDiffTime
day :: NominalDiffTime
nominalDiffTimeToMicroseconds :: NominalDiffTime -> Int
myExecutable :: IO FilePath

-- | Determine the path to this executable and save a copy to the specified
--   dir with a string appended to filename.
savedExecutable :: FilePath -> String -> IO FilePath

-- | Replaces all non-allowed characters by <tt>'_'</tt>. Allowed
--   characters are alphanumerics and .,-,_
sanitizeFileString :: String -> FilePath

-- | Truncates a string to a string of at most given length, replacing
--   dropped characters by a hash. The hash takes up 43 symbols, so asking
--   for a smaller length will still return 43 symbols.
hashTruncateString :: Int -> String -> String

-- | Synonim for <tt><a>hashTruncateString</a> 230</tt>
hashTruncateFileName :: String -> String

-- | Turn an expression with a constraint into a function of an explicit
--   dictionary
withDict :: forall (c :: Constraint) r. (c => r) -> Dict c -> r
instance GHC.Classes.Ord Hyperion.Util.Unique
instance GHC.Classes.Eq Hyperion.Util.Unique
instance GHC.Show.Show e => GHC.Show.Show (Hyperion.Util.WaitRetry e)
instance GHC.Show.Show Hyperion.Util.Unique

module Hyperion.Slurm.Sbatch

-- | Error from running <tt>sbatch</tt>. The <a>String</a>s are the
--   contents of <tt>stdout</tt> and <tt>stderr</tt> from <tt>sbatch</tt>.
data SbatchError
SbatchError :: (ExitCode, String, String) -> String -> SbatchError
[exitCodeStdinStderr] :: SbatchError -> (ExitCode, String, String)
[input] :: SbatchError -> String

-- | Type representing possible options for <tt>sbatch</tt>. Map 1-to-1 to
--   <tt>sbatch</tt> options, so see <tt>man sbatch</tt> for details.
data SbatchOptions
SbatchOptions :: Maybe Text -> Maybe FilePath -> Maybe FilePath -> Int -> Int -> NominalDiffTime -> Maybe Text -> Maybe Text -> Maybe Text -> Maybe Text -> Maybe Text -> Maybe Text -> Maybe Text -> SbatchOptions

-- | Job name ("--job-name")
[jobName] :: SbatchOptions -> Maybe Text

-- | Working directory for the job ("--D")
[chdir] :: SbatchOptions -> Maybe FilePath

-- | Where to direct <tt>stdout</tt> of the job ("--output")
[output] :: SbatchOptions -> Maybe FilePath

-- | Number of nodes ("--nodes")
[nodes] :: SbatchOptions -> Int

-- | Number of tasks per node ("--ntasks-per-node")
[nTasksPerNode] :: SbatchOptions -> Int

-- | Job time limit ("--time")
[time] :: SbatchOptions -> NominalDiffTime

-- | Memory per node, use suffix K,M,G, or T to define the units. ("--mem")
[mem] :: SbatchOptions -> Maybe Text

-- | ("--mail-type")
[mailType] :: SbatchOptions -> Maybe Text

-- | ("--mail-user")
[mailUser] :: SbatchOptions -> Maybe Text

-- | <tt>SLURM</tt> partition ("--partition")
[partition] :: SbatchOptions -> Maybe Text

-- | ("--constraint")
[constraint] :: SbatchOptions -> Maybe Text

-- | ("--account")
[account] :: SbatchOptions -> Maybe Text

-- | ("--qos")
[qos] :: SbatchOptions -> Maybe Text

-- | Default <a>SbatchOptions</a>. Request 1 task on 1 node for 24 hrs,
--   everything else unspecified.
defaultSbatchOptions :: SbatchOptions

-- | Convert <a>SbatchOptions</a> to a string of options for
--   <tt>sbatch</tt>
sBatchOptionString :: SbatchOptions -> String
sbatchOutputParser :: Parser JobId

-- | Runs <tt>sbatch</tt> on a batch file with options pulled from
--   <a>SbatchOptions</a> and script given as the <a>String</a> input
--   parameter. If <tt>sbatch</tt> exists with failure then throws
--   <a>SbatchError</a>.
sbatchScript :: SbatchOptions -> String -> IO JobId

-- | Formats <a>NominalDiffTime</a> into <tt>hh:mm:ss</tt>.
formatRuntime :: NominalDiffTime -> String

-- | Runs the command given by <a>FilePath</a> with arguments
--   <tt>[<a>Text</a>]</tt> in <tt>sbatch</tt> script via
--   <a>sbatchScript</a>. If <tt>sbatch</tt> fails then throws
--   <a>SbatchError</a>.
sbatchCommand :: SbatchOptions -> FilePath -> [Text] -> IO JobId
instance GHC.Exception.Type.Exception Hyperion.Slurm.Sbatch.SbatchError
instance GHC.Show.Show Hyperion.Slurm.Sbatch.SbatchError
instance GHC.Show.Show Hyperion.Slurm.Sbatch.SbatchOptions

module Hyperion.Slurm

module Hyperion.Remote

-- | Type for service id. <a>ServiceId</a> is typically a random string
--   that is assigned to a worker. (Maybe to other things too?)
newtype ServiceId
ServiceId :: String -> ServiceId
serviceIdToText :: ServiceId -> Text
serviceIdToString :: ServiceId -> String

-- | Type for basic master to worker messaging
data WorkerMessage
Connected :: WorkerMessage
ShutDown :: WorkerMessage
data RemoteError
RemoteError :: ServiceId -> RemoteErrorType -> RemoteError

-- | Detailed type for <a>RemoteError</a>. The constructors correspond to
--   various possible <a>AsyncResult</a>s.
data RemoteErrorType
RemoteAsyncFailed :: DiedReason -> RemoteErrorType
RemoteAsyncLinkFailed :: DiedReason -> RemoteErrorType
RemoteAsyncCancelled :: RemoteErrorType
RemoteAsyncPending :: RemoteErrorType
RemoteException :: String -> RemoteErrorType
data WorkerConnectionTimeout
WorkerConnectionTimeout :: ServiceId -> WorkerConnectionTimeout

-- | <a>WorkerLauncher</a> type parametrized by a type for job id.
data WorkerLauncher j
WorkerLauncher :: (forall b. NodeId -> ServiceId -> (j -> Process b) -> Process b) -> Maybe NominalDiffTime -> (forall b. RemoteError -> Process b -> Process b) -> WorkerLauncher j

-- | A function that launches a worker for the given <a>ServiceId</a> on
--   the master <a>NodeId</a> and supplies its job id to the given
--   continuation
[withLaunchedWorker] :: WorkerLauncher j -> forall b. NodeId -> ServiceId -> (j -> Process b) -> Process b

-- | Timeout for the worker to connect. If the worker is launched into a
--   Slurm queue, it may take a very long time to connect. In that case, it
--   is recommended to set <a>connectionTimeout</a> = <a>Nothing</a>.
[connectionTimeout] :: WorkerLauncher j -> Maybe NominalDiffTime

-- | A handler for <a>RemoteError</a>s. <a>onRemoteError</a> can do things
--   like blocking before rerunning the remote process, or simply
--   rethrowing the error.
[onRemoteError] :: WorkerLauncher j -> forall b. RemoteError -> Process b -> Process b

-- | Run a Process locally using the default <a>RemoteTable</a>.
--   Additionally allows a return value for the <a>Process</a>.
runProcessLocal :: Process a -> IO a

-- | Run a Process locally using the specified <a>RemoteTable</a>.
--   Additionally allows a return value for the <a>Process</a>.
runProcessLocalWithRT :: RemoteTable -> Process a -> IO a

-- | Spawns a new local <a>Control.Distributed.Process.Node</a> and runs
--   the given <a>Process</a> on it. Waits for the process to finish.
--   
--   Binds to the first available port by specifying port 0.
runProcessLocalWithRT_ :: RemoteTable -> Process () -> IO ()

-- | Get a hostname for the current machine that does not correspond to a
--   local network address (127.* or 10.*)
getExternalHostName :: IO String

-- | Convert a <a>Text</a> representation of <a>EndPointAddress</a> to
--   <a>NodeId</a>. The format for the end point address is "TCP host:TCP
--   port:endpoint id"
addressToNodeId :: Text -> NodeId

-- | Inverse to <a>addressToNodeId</a>
nodeIdToAddress :: NodeId -> Text
masterNodeIdLabel :: String
masterNodeIdStatic :: Static (Maybe NodeId)
getMasterNodeId :: Process (Maybe NodeId)
registerMasterNodeId :: Maybe NodeId -> RemoteTable -> RemoteTable
initWorkerRemoteTable :: Maybe NodeId -> RemoteTable

-- | The main worker process.
--   
--   Repeatedly (at most 5 times) send our own <a>ProcessId</a> and the
--   send end of a typed channel (<a>SendPort</a> <a>WorkerMessage</a>) to
--   a master node until it replies <a>Connected</a> (timeout 10 seconds
--   for each attempt). Then <a>expect</a> a <a>ShutDown</a> signal.
--   
--   While waiting, other processes will be run in a different thread,
--   invoked by master through our <a>NodeId</a> (which it extracts from
--   <a>ProcessId</a>)
worker :: NodeId -> ServiceId -> Process ()

-- | Registers (<a>register</a>) the current process under a random
--   <a>ServiceId</a>, then passes the <a>ServiceId</a> to the given
--   continuation. After the continuation returns, unregisters
--   (<a>unregister</a>) the <a>ServiceId</a>.
withServiceId :: (ServiceId -> Process a) -> Process a

-- | Start a new remote worker using <a>WorkerLauncher</a> and call a
--   continuation with the <a>NodeId</a> and <a>ServiceId</a> of that
--   worker. The continuation is run in the process that is registered
--   under the <a>ServiceId</a> (see <a>withServiceId</a>).
--   
--   Throws (<a>throwM</a>) a <a>WorkerConnectionTimeout</a> if worker
--   times out (timeout described in <a>WorkerLauncher</a>)
--   
--   The call to the user function is <a>bracket</a>ed by worker startup
--   and shutdown procedures.
withService :: Show j => WorkerLauncher j -> (NodeId -> ServiceId -> Process a) -> Process a

-- | A process that generates the <a>Closure</a> to be run on a remote
--   machine.
data SerializableClosureProcess a
SerializableClosureProcess :: Process (Closure (Process (Either String a))) -> Closure (SerializableDict (Either String a)) -> MVar (Closure (Process (Either String a))) -> SerializableClosureProcess a

-- | Process to generate a closure. This process will be run when a remote
--   location has been identified that the closure can be sent to.
[runClosureProcess] :: SerializableClosureProcess a -> Process (Closure (Process (Either String a)))

-- | Dict for seralizing the result.
[staticSDict] :: SerializableClosureProcess a -> Closure (SerializableDict (Either String a))

-- | If a remote computation fails, it may be added to the <tt>HoldMap</tt>
--   to be tried again. In that case, we don't want to evaluate
--   <a>runClosureProcess</a> again, so we use an <a>MVar</a> to memoize
--   the result of <a>runClosureProcess</a>.
[closureVar] :: SerializableClosureProcess a -> MVar (Closure (Process (Either String a)))

-- | Get closure and memoize the result
getClosure :: SerializableClosureProcess a -> Process (Closure (Process (Either String a)))

-- | The type of a function that takes a <a>SerializableClosureProcess</a>
--   and runs the <a>Closure</a> on a remote machine. In case the remote
--   machine returns a <a>Left</a> value (i.e. an error), throws this value
--   wrapped in <a>RemoteError</a> of type <a>RemoteException</a>. May
--   throw other <a>RemoteError</a>s if remote execution fails in any way.
type RemoteProcessRunner = forall a. (Binary a, Typeable a) => SerializableClosureProcess a -> Process a

-- | Starts a new remote worker and runs a user function, which is supplied
--   with <a>RemoteProcessRunner</a> for running closures on that worker.
--   If a <a>RemoteError</a> occurs, it is handled by the
--   <a>onRemoteError</a> supplied in the <a>WorkerLauncher</a>.
withRemoteRunProcess :: Show j => WorkerLauncher j -> (RemoteProcessRunner -> Process a) -> Process a

-- | Catch any exception, log it, and return as a string. In this way,
--   errors will be logged by the worker where they occurred, and also sent
--   up the tree.
tryLogException :: Process b -> Process (Either String b)

-- | Construct a SerializableClosureProcess for evaluating the closure
--   <tt>mb</tt> on a remote machine. The action <tt>mb</tt> that produces
--   the <a>Closure</a> will only be run once -- when a worker first
--   becomes available. The MVar <tt>v</tt> caches the resulting Closure
--   (so it can be re-used in the event of an error and retry), which will
--   then be sent across the network.
--   
--   Note that we wrap the given closure in tryLogException. Thus,
--   exception handling is added by catching any exception <tt>e</tt>,
--   logging <tt>e</tt> with <a>err</a> (on the worker), and returning a
--   <a>Left</a> result with the textual representation of <tt>e</tt> from
--   <a>Show</a> instance.
mkSerializableClosureProcess :: Typeable b => Closure (Dict (Serializable b)) -> Process (Closure (Process b)) -> Process (SerializableClosureProcess b)
instance Data.Binary.Class.Binary Hyperion.Remote.ServiceId
instance GHC.Generics.Generic Hyperion.Remote.ServiceId
instance GHC.Show.Show Hyperion.Remote.ServiceId
instance GHC.Classes.Eq Hyperion.Remote.ServiceId
instance Data.Binary.Class.Binary Hyperion.Remote.WorkerMessage
instance GHC.Generics.Generic Hyperion.Remote.WorkerMessage
instance GHC.Show.Show Hyperion.Remote.WorkerMessage
instance GHC.Read.Read Hyperion.Remote.WorkerMessage
instance GHC.Show.Show Hyperion.Remote.RemoteErrorType
instance GHC.Exception.Type.Exception Hyperion.Remote.RemoteError
instance GHC.Show.Show Hyperion.Remote.RemoteError
instance GHC.Exception.Type.Exception Hyperion.Remote.WorkerConnectionTimeout
instance GHC.Show.Show Hyperion.Remote.WorkerConnectionTimeout

module Hyperion.LockMap
type LockMap = TVar (Map ByteString Lock)
withLock :: (Typeable a, Binary a) => a -> Process r -> Process r
withLocks :: (Typeable a, Binary a) => [a] -> Process r -> Process r
newLockMap :: IO LockMap
registerLockMap :: LockMap -> RemoteTable -> RemoteTable

module Hyperion.HasWorkers

-- | A class for monads that can run things in the <a>Process</a> monad,
--   and have access to a <a>WorkerLauncher</a>. An instance of
--   <a>HasWorkers</a> can use <tt>remoteBind</tt> and <a>remoteEval</a> to
--   run computations in worker processes at remote locations.
class (MonadBase Process m, MonadUnliftProcess m, MonadIO m) => HasWorkers m
getWorkerLauncher :: HasWorkers m => m (WorkerLauncher JobId)

-- | A class for Monads that can run continuations in the Process monad,
--   modeled after MonadUnliftIO
--   (https:/<i>hackage.haskell.org</i>package<i>unliftio-core-0.2.0.1</i>docs/Control-Monad-IO-Unlift.html).
class MonadUnliftProcess m
withRunInProcess :: MonadUnliftProcess m => ((forall a. m a -> Process a) -> Process b) -> m b

-- | A class indicating that type <tt>env</tt> contains a
--   <a>WorkerLauncher</a>.
class HasWorkerLauncher env
toWorkerLauncher :: HasWorkerLauncher env => env -> WorkerLauncher JobId

-- | Uses the <a>WorkerLauncher</a> to get a <a>RemoteProcessRunner</a> and
--   pass it to the given continuation.
--   
--   This function is essentially a composition of <a>getWorkerLauncher</a>
--   with <a>withRemoteRunProcess</a>, lifted from <a>Process</a> to
--   <tt>m</tt> using <a>MonadUnliftProcess</a>.
--   
--   We use the machinery of <a>MonadUnliftProcess</a> because
--   <a>withRemoteRunProcess</a> expects something that runs in the
--   <a>Process</a> monad, not in <tt>m</tt>. Our main use case is when 'm
--   ~ ReaderT env Process', where <tt>env</tt> is an instance of
--   <a>HasWorkerLauncher</a>.
withRemoteRun :: HasWorkers m => (RemoteProcessRunner -> m a) -> m a

-- | Compute a closure at a remote location. The user supplies an 'm
--   (Closure (...))' which is only evaluated when a remote worker becomes
--   available (for example after the worker makes it out of the Slurm
--   queue).
remoteEvalWithDictM :: (HasWorkers m, Serializable b) => Closure (Dict (Serializable b)) -> m (Closure (Process b)) -> m b

-- | Evaluate a <a>Closure</a> at a remote location, assuming a 'Static
--   (Binary b)' instance. The Closure itself is ony computed when a worker
--   becomes available.
remoteEvalM :: (HasWorkers m, Static (Binary b), Typeable b) => m (Closure (Process b)) -> m b

-- | Evaluate a <a>Closure</a> at a remote location.
remoteEval :: (HasWorkers m, Static (Binary b), Typeable b) => Closure (Process b) -> m b
instance Hyperion.HasWorkers.HasWorkerLauncher env => Hyperion.HasWorkers.HasWorkers (Control.Monad.Trans.Reader.ReaderT env Control.Distributed.Process.Internal.Types.Process)
instance Hyperion.HasWorkers.MonadUnliftProcess Control.Distributed.Process.Internal.Types.Process
instance Hyperion.HasWorkers.MonadUnliftProcess m => Hyperion.HasWorkers.MonadUnliftProcess (Control.Monad.Trans.Reader.ReaderT r m)
instance Control.Monad.Base.MonadBase Control.Distributed.Process.Internal.Types.Process Control.Distributed.Process.Internal.Types.Process


-- | An <a>ExtVar</a> is an <a>MVar</a> that can be accessed by an external
--   client. The "host" is the machine where the underlying <a>MVar</a>
--   exists. The host can continue to use the underlying <a>MVar</a> as
--   usual. A client can interact with it via functions like
--   <a>takeExtVar</a>, <a>putExtVar</a>, <a>readExtVar</a>, etc., which
--   behave in the same way as their <a>MVar</a> counterparts. An
--   <a>ExtVar</a> can be recontstructed from its representation as a
--   String or serialized to/from Binary data (and hence sent across a
--   network).
--   
--   For an example of using an <a>ExtVar</a> as a client, look in the
--   hosts logs for a line that looks like:
--   
--   <ul>
--   <li><i>Thu 01<i>06</i>22 13:04:17</i> Made ExtVar: extVar @Int
--   "login1.cm.cluster:39443:0" "test"</li>
--   </ul>
--   
--   This shows that the host machine has made an ExtVar and it is ready to
--   be accessed by a client. Now in a GHCi session (possibly on a
--   completely different machine), you can do:
--   
--   <pre>
--   eVar = extVar @Int "login1.cm.cluster:39443:0" "test"
--   tryReadExtVarIO eVar
--   </pre>
--   
--   Just 42 &gt; modifyExtVarIO_ eVar (x -&gt; pure (x+1)) () &gt;
--   tryReadExtVarIO eVar Just 43
module Hyperion.ExtVar
data ExtVar a

-- | <a>ExtVar</a> from an address and a name. To see what arguments you
--   should pass to <a>extVar</a>, it is best to look for the "Made ExtVar"
--   entry in the log o the host machine.
extVar :: ByteString -> String -> ExtVar a

-- | Make a new <a>ExtVar</a> from an <a>MVar</a>, together with a name.
--   The host program can continue to use the <a>MVar</a> as usual. If the
--   name is not unique, a <tt>ProcessRegistrationException</tt> will be
--   thrown.
newExtVar :: (Binary a, Typeable a) => String -> MVar a -> Process (ExtVar a)

-- | Kill the server underlying the <a>ExtVar</a>. Subsequent calls from
--   clients may block indefinitely.
killExtVar :: forall a. (Binary a, Typeable a) => ExtVar a -> Process ()

-- | <a>takeExtVar</a>, etc. are analogous to <a>takeMVar</a>, etc. All
--   functions block until they receive a response from the host.
takeExtVar :: (Binary a, Typeable a) => ExtVar a -> Process a
tryTakeExtVar :: (Binary a, Typeable a) => ExtVar a -> Process (Maybe a)
putExtVar :: (Binary a, Typeable a) => ExtVar a -> a -> Process ()
tryPutExtVar :: (Binary a, Typeable a) => ExtVar a -> a -> Process Bool
readExtVar :: (Binary a, Typeable a) => ExtVar a -> Process a
tryReadExtVar :: (Binary a, Typeable a) => ExtVar a -> Process (Maybe a)
withExtVar :: (Binary a, Typeable a) => ExtVar a -> (a -> Process b) -> Process b
modifyExtVar_ :: (Binary a, Typeable a) => ExtVar a -> (a -> Process a) -> Process ()
modifyExtVar :: (Binary a, Typeable a) => ExtVar a -> (a -> Process (a, b)) -> Process b

-- | <a>IO</a> versions of <a>ExtVar</a> functions, for convenience.
takeExtVarIO :: (Binary a, Typeable a) => ExtVar a -> IO a
tryTakeExtVarIO :: (Binary a, Typeable a) => ExtVar a -> IO (Maybe a)
putExtVarIO :: (Binary a, Typeable a) => ExtVar a -> a -> IO ()
tryPutExtVarIO :: (Binary a, Typeable a) => ExtVar a -> a -> IO Bool
readExtVarIO :: (Binary a, Typeable a) => ExtVar a -> IO a
tryReadExtVarIO :: (Binary a, Typeable a) => ExtVar a -> IO (Maybe a)
withExtVarIO :: (Binary a, Typeable a) => ExtVar a -> (a -> IO b) -> IO b
modifyExtVarIO_ :: (Binary a, Typeable a) => ExtVar a -> (a -> IO a) -> IO ()
modifyExtVarIO :: (Binary a, Typeable a) => ExtVar a -> (a -> IO (a, b)) -> IO b
instance forall k (a :: k). Data.Binary.Class.Binary (Hyperion.ExtVar.ExtVar a)
instance forall k (a :: k). GHC.Generics.Generic (Hyperion.ExtVar.ExtVar a)
instance forall k (a :: k). GHC.Classes.Ord (Hyperion.ExtVar.ExtVar a)
instance forall k (a :: k). GHC.Classes.Eq (Hyperion.ExtVar.ExtVar a)
instance (Data.Binary.Class.Binary a, Data.Typeable.Internal.Typeable a) => Data.Binary.Class.Binary (Hyperion.ExtVar.ExtVarMessage a)
instance GHC.Generics.Generic (Hyperion.ExtVar.ExtVarMessage a)
instance forall k (a :: k). Data.Typeable.Internal.Typeable a => GHC.Show.Show (Hyperion.ExtVar.ExtVar a)

module Hyperion.Command

-- | Haskell representation of arguments passed to the worker process.
data Worker
Worker :: Text -> ServiceId -> FilePath -> Worker
[workerMasterAddress] :: Worker -> Text
[workerService] :: Worker -> ServiceId
[workerLogFile] :: Worker -> FilePath

-- | Parses worker command-line arguments. Essentially inverse to
--   <a>hyperionWorkerCommand</a>.
workerOpts :: Parser Worker

-- | Returns the <tt>(command, [arguments])</tt> to run the worker process
hyperionWorkerCommand :: FilePath -> NodeId -> ServiceId -> FilePath -> (String, [String])
instance GHC.Show.Show Hyperion.Command.Worker

module Hyperion.ProgramId
newtype ProgramId
ProgramId :: Text -> ProgramId
programIdToText :: ProgramId -> Text
newProgramId :: IO ProgramId
instance Data.Aeson.Types.ToJSON.ToJSON Hyperion.ProgramId.ProgramId
instance Data.Aeson.Types.FromJSON.FromJSON Hyperion.ProgramId.ProgramId
instance Data.Binary.Class.Binary Hyperion.ProgramId.ProgramId
instance GHC.Generics.Generic Hyperion.ProgramId.ProgramId
instance GHC.Classes.Ord Hyperion.ProgramId.ProgramId
instance GHC.Classes.Eq Hyperion.ProgramId.ProgramId
instance GHC.Show.Show Hyperion.ProgramId.ProgramId
instance Database.SQLite.Simple.ToField.ToField Hyperion.ProgramId.ProgramId

module Hyperion.Database.HasDB

-- | Database information datatype
data DatabaseConfig
DatabaseConfig :: Pool Connection -> ProgramId -> Int -> DatabaseConfig
[dbPool] :: DatabaseConfig -> Pool Connection
[dbProgramId] :: DatabaseConfig -> ProgramId
[dbRetries] :: DatabaseConfig -> Int

-- | <a>HasDB</a> typeclass
class HasDB env
dbConfigLens :: HasDB env => Lens' env DatabaseConfig
type Pool = Pool Connection

-- | Produces a default pool with connections to the SQLite DB in the given
--   file
newDefaultPool :: FilePath -> IO (Pool Connection)

-- | Extracts the connection pool from the environment of our monad, gets a
--   connection and runs the supplied function with it
withConnection :: forall m env a. (MonadIO m, MonadReader env m, HasDB env) => (Connection -> IO a) -> m a

-- | Tries <a>withConnection</a> until succeeds. Failure means that
--   <a>SQLError</a> is thrown during execution of the function. Otherwise
--   execution is deemed successful. The number of attempts is determined
--   by DatabaseConfig in the environment. If last attempt is a failure,
--   the last exception propagates outside of <a>withConnectionRetry</a>.
--   Uses <tt>retryRepeated</tt> internally.
withConnectionRetry :: forall m env a. (MonadIO m, MonadReader env m, HasDB env, MonadCatch m) => (Connection -> IO a) -> m a
instance Hyperion.Database.HasDB.HasDB Hyperion.Database.HasDB.DatabaseConfig

module Hyperion.Database.KeyValMap

-- | Type for <a>KeyValMap</a> holds the types of key and value, but only
--   contains <a>kvMapName</a> the name of the map
newtype KeyValMap a b
KeyValMap :: Text -> KeyValMap a b
[kvMapName] :: KeyValMap a b -> Text

-- | <a>setupKeyValTable</a> creates the table "hyperion_key_val" if it
--   doesn't yet exist. This table will hold the map-key-val entries.
--   
--   The entry format is <tt>program_id, kv_map, key, val, created_at</tt>.
--   These are the program id, map name, key, value, and timestamp,
--   respectively.
--   
--   <tt>program_id</tt> is not used in lookups
setupKeyValTable :: (MonadIO m, MonadReader env m, HasDB env, MonadCatch m) => m ()
newtype JsonField a
JsonField :: a -> JsonField a

-- | Inserts an map-key-val entry into the database.
--   
--   If fails, retries using <a>withConnectionRetry</a>
insert :: (MonadIO m, MonadReader env m, HasDB env, MonadCatch m, ToJSON a, ToJSON b) => KeyValMap a b -> a -> b -> m ()

-- | Looks up a value in the database given the map name and the key. Takes
--   the most recent matching entry according to the convention.
--   
--   If fails, retries using <a>withConnectionRetry</a>
lookup :: (MonadIO m, MonadReader env m, HasDB env, MonadCatch m, ToJSON a, Typeable b, FromJSON b) => KeyValMap a b -> a -> m (Maybe b)

-- | Returns the list of all kev-value pairs for a given map. Again only
--   keeps the latest versino of the value accroding to the convention.
--   
--   If fails, retries using <a>withConnectionRetry</a>
lookupAll :: (MonadIO m, MonadReader env m, HasDB env, MonadCatch m, Typeable a, FromJSON a, Typeable b, FromJSON b) => KeyValMap a b -> m [(a, b)]

-- | Same as <a>lookup</a> but with a default value provided
lookupDefault :: (MonadIO m, MonadReader env m, HasDB env, MonadCatch m, ToJSON a, Typeable b, FromJSON b) => KeyValMap a b -> b -> a -> m b

-- | This implements memoization using the DB. Given a function, it first
--   tries to look up the function result in the DB and if no result is
--   available, runs the function and inserts the result into the DB
memoizeWithMap :: (MonadIO m, MonadReader env m, HasDB env, MonadCatch m, ToJSON a, ToJSON b, Typeable b, FromJSON b) => KeyValMap a b -> (a -> m b) -> a -> m b
instance Data.Aeson.Types.ToJSON.ToJSON (Hyperion.Database.KeyValMap.KeyValMap a b)
instance Data.Aeson.Types.FromJSON.FromJSON (Hyperion.Database.KeyValMap.KeyValMap a b)
instance Data.Binary.Class.Binary (Hyperion.Database.KeyValMap.KeyValMap a b)
instance GHC.Generics.Generic (Hyperion.Database.KeyValMap.KeyValMap a b)
instance GHC.Show.Show (Hyperion.Database.KeyValMap.KeyValMap a b)
instance GHC.Classes.Ord (Hyperion.Database.KeyValMap.KeyValMap a b)
instance GHC.Classes.Eq (Hyperion.Database.KeyValMap.KeyValMap a b)
instance (Data.Typeable.Internal.Typeable a, Data.Aeson.Types.FromJSON.FromJSON a) => Database.SQLite.Simple.FromField.FromField (Hyperion.Database.KeyValMap.JsonField a)
instance Data.Aeson.Types.ToJSON.ToJSON a => Database.SQLite.Simple.ToField.ToField (Hyperion.Database.KeyValMap.JsonField a)
instance Database.SQLite.Simple.ToField.ToField (Hyperion.Database.KeyValMap.KeyValMap a b)

module Hyperion.Database

module Hyperion.ObjectId

-- | An identifier for an object, useful for building filenames and
--   database entries.
newtype ObjectId
ObjectId :: String -> ObjectId

-- | Convert an ObjectId to a String.
objectIdToString :: ObjectId -> String

-- | Convert an ObjectId to Text.
objectIdToText :: ObjectId -> Text

-- | The ObjectId of an object is the result of <a>hashBase64Safe</a>. The
--   first time <a>getObjectId</a> is called, it comptues the ObjectId and
--   stores it in the database before returning it. Subsequent calls read
--   the value from the database.
getObjectId :: (Binary a, Typeable a, ToJSON a, HasDB env, MonadReader env m, MonadIO m, MonadCatch m) => a -> m ObjectId
instance Data.Aeson.Types.ToJSON.ToJSON Hyperion.ObjectId.ObjectId
instance Data.Aeson.Types.FromJSON.FromJSON Hyperion.ObjectId.ObjectId
instance Data.Binary.Class.Binary Hyperion.ObjectId.ObjectId
instance GHC.Generics.Generic Hyperion.ObjectId.ObjectId
instance GHC.Classes.Ord Hyperion.ObjectId.ObjectId
instance GHC.Classes.Eq Hyperion.ObjectId.ObjectId

module Hyperion.WorkerCpuPool

-- | A newtype for the number of available CPUs
newtype NumCPUs
NumCPUs :: Int -> NumCPUs

-- | The <a>WorkerCpuPool</a> type, contaning a map of available CPU
--   resources
data WorkerCpuPool
WorkerCpuPool :: TVar (Map WorkerAddr NumCPUs) -> WorkerCpuPool
[cpuMap] :: WorkerCpuPool -> TVar (Map WorkerAddr NumCPUs)

-- | <a>newWorkerCpuPool</a> creates a new <a>WorkerCpuPool</a> from a
--   <a>Map</a>.
newWorkerCpuPool :: Map WorkerAddr NumCPUs -> IO WorkerCpuPool

-- | Gets a list of all <a>WorkerAddr</a> registered in
--   <a>WorkerCpuPool</a>
getAddrs :: WorkerCpuPool -> IO [WorkerAddr]

-- | A <a>WorkerAddr</a> representing a node address. Can be a remote node
--   or the local node
data WorkerAddr
LocalHost :: String -> WorkerAddr
RemoteAddr :: String -> WorkerAddr

-- | Reads the system environment to obtain the list of nodes allocated to
--   the job. If the local node is in the list, then records it too, as
--   <a>LocalHost</a>.
getSlurmAddrs :: IO [WorkerAddr]

-- | Reads the system environment to determine the number of CPUs available
--   on each node (the same number on each node), and creates a new
--   <a>WorkerCpuPool</a> for the <tt>[<a>WorkerAddr</a>]</tt> assuming
--   that all CPUs are available.
newJobPool :: [WorkerAddr] -> IO WorkerCpuPool

-- | Finds the worker with the most available CPUs and runs the given
--   routine with the address of that worker. Blocks if the number of
--   available CPUs is less than the number requested.
withWorkerAddr :: (MonadIO m, MonadMask m) => WorkerCpuPool -> NumCPUs -> (WorkerAddr -> m a) -> m a
data SSHError
SSHError :: String -> (ExitCode, String, String) -> SSHError

-- | The type for the command used to run <tt>ssh</tt>. If a <a>Just</a>
--   value, then the first <a>String</a> gives the name of <tt>ssh</tt>
--   executable, e.g. <tt>"ssh"</tt>, and the list of <a>String</a>s gives
--   the options to pass to <tt>ssh</tt>. For example, with
--   <a>SSHCommand</a> given by <tt>("XX", ["-a", "-b"])</tt>, <tt>ssh</tt>
--   is run as
--   
--   <pre>
--   XX -a -b &lt;addr&gt; &lt;command&gt;
--   </pre>
--   
--   where <tt>&lt;addr&gt;</tt> is the remote address and
--   <tt>&lt;command&gt;</tt> is the command we need to run there.
--   
--   The value of <a>Nothing</a> is equivalent to using
--   
--   <pre>
--   ssh -f -o "UserKnownHostsFile /dev/null" &lt;addr&gt; &lt;command&gt;
--   </pre>
--   
--   We need <tt>-o "..."</tt> option to deal with host key verification
--   failures. We use <tt>-f</tt> to force <tt>ssh</tt> to go to the
--   background before executing the <tt>sh</tt> call. This allows for a
--   faster return from <a>readCreateProcessWithExitCode</a>.
--   
--   Note that <tt>"UserKnownHostsFile /dev/null"</tt> doesn't seem to work
--   on Helios. Using instead <tt>"StrictHostKeyChecking=no"</tt> seems to
--   work.
type SSHCommand = Maybe (String, [String])

-- | Runs a given command on remote host (with address given by the first
--   <a>String</a>) with the given arguments via <tt>ssh</tt> using the
--   <a>SSHCommand</a>. Makes at most 10 attempts via <a>retryRepeated</a>.
--   If fails, propagates <a>SSHError</a> outside.
--   
--   <tt>ssh</tt> needs to be able to authenticate on the remote node
--   without a password. In practice you will probably need to set up
--   public key authentiticaion.
--   
--   <tt>ssh</tt> is invoked to run <tt>sh</tt> that calls <tt>nohup</tt>
--   to run the supplied command in background.
sshRunCmd :: String -> SSHCommand -> (String, [String]) -> IO ()
instance GHC.Num.Num Hyperion.WorkerCpuPool.NumCPUs
instance GHC.Classes.Ord Hyperion.WorkerCpuPool.NumCPUs
instance GHC.Classes.Eq Hyperion.WorkerCpuPool.NumCPUs
instance GHC.Show.Show Hyperion.WorkerCpuPool.WorkerAddr
instance GHC.Classes.Ord Hyperion.WorkerCpuPool.WorkerAddr
instance GHC.Classes.Eq Hyperion.WorkerCpuPool.WorkerAddr
instance GHC.Exception.Type.Exception Hyperion.WorkerCpuPool.SSHError
instance GHC.Show.Show Hyperion.WorkerCpuPool.SSHError

module Hyperion.Cluster

-- | Type containing information about our program
data ProgramInfo
ProgramInfo :: ProgramId -> FilePath -> FilePath -> FilePath -> SSHCommand -> ProgramInfo
[programId] :: ProgramInfo -> ProgramId
[programDatabase] :: ProgramInfo -> FilePath
[programLogDir] :: ProgramInfo -> FilePath
[programDataDir] :: ProgramInfo -> FilePath
[programSSHCommand] :: ProgramInfo -> SSHCommand

-- | The environment for <a>Cluster</a> monad.
data ClusterEnv
ClusterEnv :: (SbatchOptions -> ProgramInfo -> WorkerLauncher JobId) -> ProgramInfo -> SbatchOptions -> Pool -> Int -> LockMap -> ClusterEnv
[clusterWorkerLauncher] :: ClusterEnv -> SbatchOptions -> ProgramInfo -> WorkerLauncher JobId
[clusterProgramInfo] :: ClusterEnv -> ProgramInfo
[clusterJobOptions] :: ClusterEnv -> SbatchOptions
[clusterDatabasePool] :: ClusterEnv -> Pool
[clusterDatabaseRetries] :: ClusterEnv -> Int
[clusterLockMap] :: ClusterEnv -> LockMap
class HasProgramInfo a
toProgramInfo :: HasProgramInfo a => a -> ProgramInfo

-- | The <a>Cluster</a> monad. It is simply <a>Process</a> with
--   <a>ClusterEnv</a> environment.
type Cluster = ReaderT ClusterEnv Process

-- | Type representing resources for an MPI job.
data MPIJob
MPIJob :: Int -> Int -> MPIJob
[mpiNodes] :: MPIJob -> Int
[mpiNTasksPerNode] :: MPIJob -> Int
runCluster :: ClusterEnv -> Cluster a -> IO a
modifyJobOptions :: (SbatchOptions -> SbatchOptions) -> ClusterEnv -> ClusterEnv
setJobOptions :: SbatchOptions -> ClusterEnv -> ClusterEnv
setJobTime :: NominalDiffTime -> ClusterEnv -> ClusterEnv
setJobMemory :: Text -> ClusterEnv -> ClusterEnv
setJobType :: MPIJob -> ClusterEnv -> ClusterEnv
setSlurmPartition :: Text -> ClusterEnv -> ClusterEnv
setSlurmConstraint :: Text -> ClusterEnv -> ClusterEnv
setSlurmAccount :: Text -> ClusterEnv -> ClusterEnv
setSlurmQos :: Text -> ClusterEnv -> ClusterEnv

-- | The default number of retries to use in <tt>withConnectionRetry</tt>.
--   Set to 20.
defaultDBRetries :: Int
dbConfigFromProgramInfo :: ProgramInfo -> IO DatabaseConfig
runDBWithProgramInfo :: ProgramInfo -> ReaderT DatabaseConfig IO a -> IO a
slurmWorkerLauncher :: Maybe Text -> FilePath -> HoldMap -> Int -> TokenPool -> SbatchOptions -> ProgramInfo -> WorkerLauncher JobId

-- | Construct a working directory for the given object, using its
--   ObjectId. Will be a subdirectory of <a>programDataDir</a>. Created
--   automatically, and saved in the database.
newWorkDir :: (Binary a, Typeable a, ToJSON a, HasProgramInfo env, HasDB env, MonadReader env m, MonadIO m, MonadCatch m) => a -> m FilePath
instance Data.Aeson.Types.ToJSON.ToJSON Hyperion.Cluster.ProgramInfo
instance Data.Aeson.Types.FromJSON.FromJSON Hyperion.Cluster.ProgramInfo
instance Data.Binary.Class.Binary Hyperion.Cluster.ProgramInfo
instance GHC.Generics.Generic Hyperion.Cluster.ProgramInfo
instance GHC.Show.Show Hyperion.Cluster.ProgramInfo
instance GHC.Classes.Ord Hyperion.Cluster.ProgramInfo
instance GHC.Classes.Eq Hyperion.Cluster.ProgramInfo
instance Data.Aeson.Types.ToJSON.ToJSON Hyperion.Cluster.MPIJob
instance Data.Aeson.Types.FromJSON.FromJSON Hyperion.Cluster.MPIJob
instance Data.Binary.Class.Binary Hyperion.Cluster.MPIJob
instance GHC.Generics.Generic Hyperion.Cluster.MPIJob
instance GHC.Show.Show Hyperion.Cluster.MPIJob
instance GHC.Classes.Ord Hyperion.Cluster.MPIJob
instance GHC.Classes.Eq Hyperion.Cluster.MPIJob
instance Hyperion.Cluster.HasProgramInfo Hyperion.Cluster.ClusterEnv
instance Hyperion.Database.HasDB.HasDB Hyperion.Cluster.ClusterEnv
instance Hyperion.HasWorkers.HasWorkerLauncher Hyperion.Cluster.ClusterEnv
instance Hyperion.Static.Class.Static (Data.Binary.Class.Binary Hyperion.Cluster.ProgramInfo)

module Hyperion.Job

-- | The environment type for <a>Job</a> monad.
data JobEnv
JobEnv :: DatabaseConfig -> NumCPUs -> NumCPUs -> ProgramInfo -> (NumCPUs -> WorkerLauncher JobId) -> JobEnv

-- | <a>DatabaseConfig</a> for the database to use
[jobDatabaseConfig] :: JobEnv -> DatabaseConfig

-- | Number of CPUs available on each node in the job
[jobNodeCpus] :: JobEnv -> NumCPUs

-- | Number of CPUs to use for running remote functions
[jobTaskCpus] :: JobEnv -> NumCPUs

-- | <a>ProgramInfo</a> inherited from the master
[jobProgramInfo] :: JobEnv -> ProgramInfo

-- | a <a>WorkerLauncher</a> that runs workers with the given number of
--   CPUs allocated
[jobTaskLauncher] :: JobEnv -> NumCPUs -> WorkerLauncher JobId

-- | Configuration for <a>withNodeLauncher</a>.
data NodeLauncherConfig
NodeLauncherConfig :: FilePath -> SSHCommand -> NodeLauncherConfig

-- | The directory to which the workers shall log.
[nodeLogDir] :: NodeLauncherConfig -> FilePath

-- | The command used to run <tt>ssh</tt>. See <a>SSHCommand</a> for
--   description.
[nodeSshCmd] :: NodeLauncherConfig -> SSHCommand

-- | <a>Job</a> monad is simply <a>Process</a> with <a>JobEnv</a>
--   environment.
type Job = ReaderT JobEnv Process

-- | Changses <a>jobTaskCpus</a> in <a>JobEnv</a>
setTaskCpus :: NumCPUs -> JobEnv -> JobEnv

-- | Runs the <a>Job</a> monad assuming we are inside a SLURM job. In
--   practice it just fills in the environment <a>JobEnv</a> and calls
--   <a>runReaderT</a>. The environment is mostly constructed from
--   <tt>SLURM</tt> environment variables and <a>ProgramInfo</a>. The
--   exceptions to these are <a>jobTaskCpus</a>, which is set to
--   <tt><a>NumCPUs</a> 1</tt>, and <a>jobTaskLauncher</a>, which is
--   created by <a>withPoolLauncher</a>. The log file has the form
--   "/a/b/c/progid/serviceid.log" . The log directory for the node is
--   obtained by dropping the .log extension: "/a/b/c/progid/serviceid"
runJobSlurm :: ProgramInfo -> Job a -> Process a

-- | Runs the <a>Job</a> locally in IO without using any information from a
--   SLURM environment, with some basic default settings. This function is
--   provided primarily for testing.
runJobLocal :: ProgramInfo -> Job a -> IO a

-- | <a>WorkerLauncher</a> that uses the supplied command runner to launch
--   workers. Sets <a>connectionTimeout</a> to <a>Nothing</a>. Uses the
--   <a>ServiceId</a> supplied to <a>withLaunchedWorker</a> to construct
--   <a>JobId</a> (through <a>JobName</a>). The supplied <a>FilePath</a> is
--   used as log directory for the worker, with the log file name derived
--   from <a>ServiceId</a>.
workerLauncherWithRunCmd :: MonadIO m => FilePath -> ((String, [String]) -> Process ()) -> m (WorkerLauncher JobId)

-- | Given a <a>NodeLauncherConfig</a> and a <a>WorkerAddr</a> runs the
--   continuation <a>Maybe</a> passing it a pair <tt>(<a>WorkerAddr</a>,
--   <a>WorkerLauncher</a> <a>JobId</a>)</tt>. Passing <a>Nothing</a>
--   repersents <tt>ssh</tt> failure.
--   
--   While <a>WorkerAddr</a> is preserved, the passed <a>WorkerLauncher</a>
--   launches workers on the node at <a>WorkerAddr</a>. The launcher is
--   derived from <a>workerLauncherWithRunCmd</a>, where command runner is
--   either local shell (if <a>WorkerAddr</a> is <tt>LocalHost</tt>) or a
--   <tt>RemoteFunction</tt> that runs the local shell on <a>WorkerAddr</a>
--   via <a>withRemoteRunProcess</a> and related functions (if
--   <a>WorkerAddr</a> is <tt>RemoteAddr</tt>).
--   
--   Note that the process of launching a worker on the remote node will
--   actually spawn an "utility" worker there that will launch all new
--   workers in the continuation. This utility worker will have its log in
--   the log dir, identified by some random <a>ServiceId</a> and put
--   messages like "Running command ...".
--   
--   The reason that utility workers are used on each Job node is to
--   minimize the number of calls to <tt>ssh</tt> or <tt>srun</tt>. The
--   naive way to launch workers in the <a>Job</a> monad would be to
--   determine what node they should be run on, and run the hyperion worker
--   command via <tt>ssh</tt>. Unfortunately, many clusters have flakey
--   <tt>ssh</tt> configurations that start throwing errors if <tt>ssh</tt>
--   is called too many times in quick succession. <tt>ssh</tt> also has to
--   perform authentication. Experience shows that <tt>srun</tt> is also
--   not a good solution to this problem, since <tt>srun</tt> talks to
--   <tt>SLURM</tt> to manage resources and this can take a long time,
--   affecting performance. Instead, we <tt>ssh</tt> exactly once to each
--   node in the Job (besides the head node), and start utility workers
--   there. These workers can then communicate with the head node via the
--   usual machinery of <tt>hyperion</tt> --- effectively, we keep a
--   connection open to each node so that we no longer have to use
--   <tt>ssh</tt>.
withNodeLauncher :: NodeLauncherConfig -> WorkerAddr -> (Maybe (WorkerAddr, WorkerLauncher JobId) -> Process a) -> Process a

-- | Run the given command in a child thread. Async.link ensures that
--   exceptions from the child are propagated to the parent.
--   
--   NB: Previously, this function used <a>createProcess</a> and discarded
--   the resulting <tt>ProcessHandle</tt>. This could result in
--   "insufficient resource" errors for OS threads. Hopefully the current
--   implementation avoids this problem.
runCmdLocalAsync :: (String, [String]) -> IO ()

-- | Run the given command and log the command. This is suitable for
--   running on remote machines so we can keep track of what is being run
--   where.
runCmdLocalLog :: (String, [String]) -> IO ()

-- | Takes a <a>NodeLauncherConfig</a> and a list of addresses. Tries to
--   start "worker-launcher" workers on these addresses (see
--   <a>withNodeLauncher</a>). Discards addresses on which the this fails.
--   From remaining addresses builds a worker CPU pool. The continuation is
--   then passed a function that launches workers in this pool. The
--   <tt>WorkerLaunchers</tt> that continuation gets have
--   <a>connectionTimeout</a> to <a>Nothing</a>.
withPoolLauncher :: NodeLauncherConfig -> [WorkerAddr] -> ((NumCPUs -> WorkerLauncher JobId) -> Process a) -> Process a
remoteEvalJobM :: (Static (Binary b), Typeable b) => Cluster (Closure (Job b)) -> Cluster b
remoteEvalJob :: (Static (Binary b), Typeable b) => Closure (Job b) -> Cluster b
instance Hyperion.Cluster.HasProgramInfo Hyperion.Job.JobEnv
instance Hyperion.Database.HasDB.HasDB Hyperion.Job.JobEnv
instance Hyperion.HasWorkers.HasWorkerLauncher Hyperion.Job.JobEnv

module Hyperion.Config

-- | Global configuration for <a>Hyperion</a> cluster.
data HyperionConfig
HyperionConfig :: SbatchOptions -> Maybe Int -> FilePath -> FilePath -> FilePath -> FilePath -> FilePath -> Maybe FilePath -> Maybe FilePath -> SSHCommand -> Maybe Text -> HyperionConfig

-- | Default options to use for <tt>sbatch</tt> submissions
[defaultSbatchOptions] :: HyperionConfig -> SbatchOptions

-- | Maximum number of jobs to submit at a time
[maxSlurmJobs] :: HyperionConfig -> Maybe Int

-- | Base directory for working dirs produced by <a>newWorkDir</a>
[dataDir] :: HyperionConfig -> FilePath

-- | Base directory for all the log files
[logDir] :: HyperionConfig -> FilePath

-- | Base directory for databases
[databaseDir] :: HyperionConfig -> FilePath

-- | Base directory for copies of the main executable
[execDir] :: HyperionConfig -> FilePath

-- | Base directory for SLURM job files
[jobDir] :: HyperionConfig -> FilePath

-- | The command to run the main executable. Automatic if <a>Nothing</a>
--   (see <a>newClusterEnv</a>)
[hyperionCommand] :: HyperionConfig -> Maybe FilePath

-- | The database from which to initiate the program database
[initialDatabase] :: HyperionConfig -> Maybe FilePath

-- | The command used to run <tt>ssh</tt> on nodes. Usually can be safely
--   set to <a>Nothing</a>. See <a>SSHCommand</a> for details.
[sshRunCommand] :: HyperionConfig -> SSHCommand

-- | Email address for cluster notifications from hyperion. Nothing means
--   no emails will be sent. Note that this setting can be different from
--   the one in defaultSbatchOptions, which controls notifications from
--   SLURM.
[emailAddr] :: HyperionConfig -> Maybe Text

-- | Default configuration, with all paths built form a single
--   <tt>baseDirectory</tt>
defaultHyperionConfig :: FilePath -> HyperionConfig

-- | Takes <a>HyperionConfig</a> and returns <a>ClusterEnv</a>, the path to
--   the executable, and a new 'HoldMap.
--   
--   Things to note:
--   
--   <ul>
--   <li><a>programId</a> is generated randomly.</li>
--   <li>If <a>hyperionCommand</a> is specified in <a>HyperionConfig</a>,
--   then <tt>hyperionExec</tt> == <a>hyperionCommand</a>. Otherwise the
--   running executable is copied to <a>execDir</a> with a unique name, and
--   that is used as <tt>hyperionExec</tt>.</li>
--   <li><a>newDatabasePath</a> is used to determine <a>programDatabase</a>
--   from <a>initialDatabase</a> and <a>databaseDir</a>,
--   <a>programId</a>.</li>
--   <li><a>timedProgramDir</a> is used to determine <a>programLogDir</a>
--   and <a>programDataDir</a> from the values in <a>HyperionConfig</a> and
--   <a>programId</a>.</li>
--   <li><a>slurmWorkerLauncher</a> is used for
--   <a>clusterWorkerLauncher</a></li>
--   <li><a>clusterDatabaseRetries</a> is set to
--   <a>defaultDBRetries</a>.</li>
--   </ul>
newClusterEnv :: HyperionConfig -> HoldMap -> Int -> IO (ClusterEnv, FilePath)

-- | Returns the path to a new database, given <a>Maybe</a> inital database
--   filepath and base directory
--   
--   If <a>ProgramId</a> id is <tt>XXXXX</tt> and initial database filename
--   is <tt>original.sqlite</tt>, then the new filename is
--   <tt>original-XXXXX.sqlite</tt>. If initial database path is
--   <a>Nothing</a>, then the filename is <tt>XXXXX.sqlite</tt>.
--   
--   The path is in subdirectory <tt>YYYY-mm</tt> (determined by current
--   date) of base directory.
--   
--   If inital database is given, then the new database is initilized with
--   its contents.
newDatabasePath :: Maybe FilePath -> FilePath -> ProgramId -> IO FilePath

-- | Given base directory and <a>ProgramId</a> (<tt>==XXXXX</tt>), returns
--   the <tt>YYYY-mm/XXXXX</tt> subdirectory of the base directory
--   (determined by current date).
timedProgramDir :: FilePath -> ProgramId -> IO FilePath

module Hyperion.Main

-- | The type for command-line options to <a>hyperionMain</a>. Here
--   <tt>a</tt> is the type for program-specific options. In practice we
--   want <tt>a</tt> to be an instance of <a>Show</a>
data HyperionOpts a

-- | Constructor for the case of a master process, holds program-specific
--   options
HyperionMaster :: a -> HyperionOpts a

-- | Constructor for the case of a worker process, holds <a>Worker</a>
--   which is parsed by <a>workerOpts</a>
HyperionWorker :: Worker -> HyperionOpts a

-- | Main command-line option parser for <a>hyperionMain</a>. Returns a
--   <a>Parser</a> that supports commands "worker" and "master", and uses
--   <a>workerOpts</a> or the supplied parser, respectively, to parse the
--   remaining options
hyperionOpts :: Parser a -> Parser (HyperionOpts a)

-- | Same as <a>hyperionOpts</a> but with added <tt>--help</tt> option and
--   wrapped into <a>ParserInfo</a> (by adding program description). This
--   now can be used in <a>execParser</a> from <a>Options.Applicative</a>.
opts :: Parser a -> ParserInfo (HyperionOpts a)

-- | <a>hyperionMain</a> produces an <tt><a>IO</a> ()</tt> action that runs
--   <tt>hyperion</tt> and can be assigned to <tt>main</tt>. It performs
--   the following actions
--   
--   <ol>
--   <li>If command-line arguments start with command <tt>master</tt>
--   then<ul><li>Uses the supplied parser to parse the remaining options
--   into type <tt>a</tt></li><li>Uses the supplied function to extract
--   <a>HyperionConfig</a> from <tt>a</tt></li><li>The data in
--   <a>HyperionConfig</a> is then used for all following
--   actions</li><li>Depending on <a>HyperionConfig</a>, extra actions may
--   be performed, see <a>newClusterEnv</a>.</li><li>Starts a log in
--   <tt>stderr</tt>, and then redirects it to a file</li><li>Starts a hold
--   server from <a>Hyperion.HoldServer</a></li><li>Uses
--   <a>setupKeyValTable</a> to setup a <a>Hyperion.Database.KeyValMap</a>
--   in the program database</li><li>Runs the supplied <tt><a>Cluster</a>
--   ()</tt> action</li><li>Cleans up the copy of the executable, if exists
--   (see <a>newClusterEnv</a>).</li></ul></li>
--   <li>If command-line arguments start with command <tt>worker</tt>
--   then<ul><li>Extracts <a>Worker</a> from the rest of the command-line
--   args.</li><li>Logs <tt>ServiceId</tt> of the worker and the system
--   environment to worker log file.</li><li>Runs <tt><a>worker</a> (...)
--   :: <tt>Process</tt> ()</tt> that connects to the master and waits for
--   a <a>ShutDown</a> message (see <a>worker</a> for details). While
--   waiting, the master can run computations on the node. Low-level
--   functions for this are implemented in <a>Hyperion.Remote</a>, and some
--   higher-level functions in <a>Hyperion.HasWorkers</a></li></ul></li>
--   </ol>
hyperionMain :: Show a => Parser a -> (a -> HyperionConfig) -> (a -> Cluster ()) -> IO ()

module Hyperion
